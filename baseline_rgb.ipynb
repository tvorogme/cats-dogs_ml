{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from PIL import Image,ImageOps\n",
    "from cats_dogs_preproc import *\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "\n",
    "theano.config.exception_verbosity = \"high\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Р’РѕР»С€РµР±РЅС‹Рµ РїРµСЂРµРјРµРЅРЅС‹Рµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# РР·РѕР±СЂР°Р¶РµРЅРёСЏ\n",
    "IMG_GEN_IMG = False\n",
    "IMG_HEIGHT = 60\n",
    "IMG_WIDTH = 60\n",
    "IMG_GRAY = True\n",
    "\n",
    "# РћР±СѓС‡Р°РµРј\n",
    "NUM_EPOCH = 100\n",
    "BATCH_SIZE = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### РџСЂРµРїР°СЂРёСЂСѓРµРј РёР·РѕР±СЂР°Р¶РµРЅРёРµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed images:  1000\n",
      "processed images:  2000\n",
      "processed images:  3000\n",
      "processed images:  4000\n",
      "processed images:  5000\n",
      "processed images:  6000\n",
      "processed images:  7000\n",
      "processed images:  8000\n",
      "processed images:  9000\n",
      "processed images:  10000\n",
      "processed images:  11000\n",
      "processed images:  12000\n",
      "processed images:  13000\n",
      "processed images:  14000\n",
      "processed images:  15000\n",
      "processed images:  16000\n",
      "processed images:  17000\n",
      "processed images:  18000\n",
      "processed images:  19000\n",
      "processed images:  20000\n",
      "processed images:  21000\n",
      "processed images:  22000\n",
      "processed images:  23000\n",
      "processed images:  24000\n",
      "processed images:  25000\n",
      "processed images:  1000\n",
      "processed images:  2000\n",
      "processed images:  3000\n",
      "processed images:  4000\n",
      "processed images:  5000\n",
      "processed images:  6000\n",
      "processed images:  7000\n",
      "processed images:  8000\n",
      "processed images:  9000\n",
      "processed images:  10000\n",
      "processed images:  11000\n",
      "processed images:  12000\n",
      "processed images:  13000\n",
      "processed images:  14000\n",
      "processed images:  15000\n",
      "processed images:  16000\n",
      "processed images:  17000\n",
      "processed images:  18000\n",
      "processed images:  19000\n",
      "processed images:  20000\n",
      "processed images:  21000\n",
      "processed images:  22000\n",
      "processed images:  23000\n",
      "processed images:  24000\n",
      "processed images:  25000\n"
     ]
    }
   ],
   "source": [
    "X,y = load_data(True, IMG_HEIGHT, IMG_WIDTH, IMG_GRAY)\n",
    "\n",
    "X_new,y_new = load_data(True, IMG_HEIGHT, IMG_WIDTH,not IMG_GRAY)\n",
    "\n",
    "# Р РµР¶РµРј РґР°С‚Сѓ\n",
    "TRAIN_SIZE = (len(X)*4)/5\n",
    "VAL_SIZE = len(X)/10\n",
    "TEST_SIZE = len(X) - TRAIN_SIZE - VAL_SIZE\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = prepar_data(X, y, TRAIN_SIZE, VAL_SIZE, TEST_SIZE)\n",
    "\n",
    "X_train_new, y_train_new, \\\n",
    "X_val_new, y_val_new, X_test_new, y_test_new = prepar_data(X_new, y_new, TRAIN_SIZE, VAL_SIZE, TEST_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 1, 60, 60) (25000, 3, 60, 60)\n"
     ]
    }
   ],
   "source": [
    "input_X = T.tensor4(\"X cat/dog image\")\n",
    "target_y = T.vector(\"target Y cat/dog\",dtype='int32')\n",
    "input_shape = [None] + list(X.shape[1:])\n",
    "\n",
    "input_X_new = T.tensor4(\"X cat/dog image gray\")\n",
    "input_shape_new = [None] + list(X_new.shape[1:])\n",
    "\n",
    "print X.shape, X_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Р Р°Р·РІР°СЂР°С‡РёРІР°РµРј СЃРµС‚СЊ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "#    FIRST INPUT\n",
    "#\n",
    "#\n",
    "\n",
    "input_layer = lasagne.layers.InputLayer(shape = input_shape,input_var=input_X)\n",
    "\n",
    "conv_1 = lasagne.layers.Conv2DLayer(input_layer, num_filters = 10,filter_size = (2,2),\n",
    "                                    nonlinearity= lasagne.nonlinearities.very_leaky_rectify)\n",
    "\n",
    "batch_1 = lasagne.layers.batch_norm(conv_1)\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "#\n",
    "#    SECOND INPUT\n",
    "#\n",
    "\n",
    "input_layer2 = lasagne.layers.InputLayer(shape = input_shape_new,input_var=input_X_new)\n",
    "\n",
    "conv_2 = lasagne.layers.Conv2DLayer(input_layer2, num_filters = 10,filter_size = (2,2),\n",
    "                                    nonlinearity= lasagne.nonlinearities.very_leaky_rectify)\n",
    "\n",
    "batch_2 = lasagne.layers.batch_norm(conv_2)\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "#\n",
    "#     OUTPUT\n",
    "#\n",
    "\n",
    "\n",
    "con = lasagne.layers.ConcatLayer([batch_1, batch_2])\n",
    "\n",
    "pool_1 = lasagne.layers.MaxPool2DLayer(con, pool_size = 5)\n",
    "\n",
    "\n",
    "batch_3 = lasagne.layers.batch_norm(pool_1)\n",
    "\n",
    "\n",
    "dense_output = lasagne.layers.DenseLayer(batch_3, num_units = 2, nonlinearity = lasagne.nonlinearities.softmax,\n",
    "                                        name='output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#РїСЂРµРґСЃРєР°Р·Р°РЅРёРµ РЅРµР№СЂРѕРЅРєРё (theano-РїСЂРµРѕР±СЂР°Р·РѕРІР°РЅРёРµ)\n",
    "y_predicted = lasagne.layers.get_output(dense_output)\n",
    "\n",
    "#РІСЃРµ РІРµСЃР° РЅРµР№СЂРѕРЅРєРё (shared-РїРµСЂРµРјРµРЅРЅС‹Рµ)\n",
    "all_weights = lasagne.layers.get_all_params(dense_output,trainable=True)\n",
    "\n",
    "#С„СѓРЅРєС†РёСЏ РѕС€РёР±РєРё - СЃСЂРµРґРЅСЏСЏ РєСЂРѕСЃСЃСЌРЅС‚СЂРѕРїРёСЏ\n",
    "loss = lasagne.objectives.categorical_crossentropy(y_predicted,target_y).mean()\n",
    "\n",
    "#СЂРµРіСѓР»СЏСЂРёР·Р°С†РёСЏ, РµСЃР»Рё Р·Р°С…РѕС‚РёС‚Рµ\n",
    "loss += lasagne.regularization.l2(loss)\n",
    "\n",
    "#<РІРѕР·РјРѕР¶РЅРѕ РґРѕР±Р°РІРёС‚СЊ СЂРµРіСѓР»СЏСЂРёР·Р°С‚РѕСЂ>\n",
    "accuracy = lasagne.objectives.categorical_accuracy(y_predicted,target_y).mean()\n",
    "\n",
    "#СЃСЂР°Р·Сѓ РїРѕСЃС‡РёС‚Р°С‚СЊ СЃР»РѕРІР°СЂСЊ РѕР±РЅРѕРІР»С‘РЅРЅС‹С… Р·РЅР°С‡РµРЅРёР№ СЃ С€Р°РіРѕРј РїРѕ РіСЂР°РґРёРµРЅС‚Сѓ, РєР°Рє СЂР°РЅСЊС€Рµ\n",
    "updates_sgd = lasagne.updates.adamax(loss,all_weights,learning_rate = 0.0001)\n",
    "\n",
    "#С„СѓРЅРєС†РёСЏ, РєРѕС‚РѕСЂР°СЏ РѕР±СѓС‡Р°РµС‚ СЃРµС‚СЊ РЅР° 1 С€Р°Рі Рё РІРѕР·РІСЂР°С‰Р°С‰РµС‚ Р·РЅР°С‡РµРЅРёРµ С„СѓРЅРєС†РёРё РїРѕС‚РµСЂСЊ Рё С‚РѕС‡РЅРѕСЃС‚Рё\n",
    "train_fun = theano.function([input_X, input_X_new, target_y],[loss,accuracy],updates= updates_sgd)\n",
    "\n",
    "#С„СѓРЅРєС†РёСЏ, РєРѕС‚РѕСЂР°СЏ СЃС‡РёС‚Р°РµС‚ С‚РѕС‡РЅРѕСЃС‚СЊ\n",
    "accuracy_fun = theano.function([input_X, input_X_new, target_y],accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### РћР±СѓС‡Р°РµРј"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = []\n",
    "test = []\n",
    "for epoch in range(NUM_EPOCH):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0\n",
    "    train_acc = 0\n",
    "    train_batches = 0\n",
    "    start_time = time.time()\n",
    "    for batch in iterate_minibatches(X_train, X_train_new, y_train, BATCH_SIZE):\n",
    "        inputs, inputs_new, targets = batch\n",
    "        train_err_batch, train_acc_batch= train_fun(inputs, inputs_new, targets)\n",
    "        train_err += train_err_batch\n",
    "        train_acc += train_acc_batch\n",
    "        train_batches += 1\n",
    "\n",
    "    # And a full pass over the validation data:\n",
    "    val_acc = 0\n",
    "    val_batches = 0\n",
    "    for batch in iterate_minibatches(X_val, X_val_new, y_val, BATCH_SIZE):\n",
    "        inputs, inputs_new, targets = batch\n",
    "        val_acc += accuracy_fun(inputs, inputs_new, targets)\n",
    "        val_batches += 1\n",
    "\n",
    "    \n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, NUM_EPOCH, time.time() - start_time))\n",
    "\n",
    "    print(\"  training loss (in-iteration):\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "    print(\"  train accuracy:\\t\\t{:.2f} %\".format(\n",
    "        train_acc / train_batches * 100))\n",
    "    train.append(train_acc / train_batches * 100)\n",
    "    print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "        val_acc / val_batches * 100))\n",
    "    test.append(val_acc / val_batches * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_acc = 0\n",
    "test_batches = 0\n",
    "for batch in iterate_minibatches(X_test, y_test, 500):\n",
    "    inputs, targets = batch\n",
    "    acc = accuracy_fun(inputs, targets)\n",
    "    test_acc += acc\n",
    "    test_batches += 1\n",
    "print(\"Final results:\")\n",
    "print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "    test_acc / test_batches * 100))\n",
    "# if(max_accuracy<(test_acc / test_batches * 100)):\n",
    "#     max_accuracy = test_acc / test_batches * 100\n",
    "\n",
    "# max_accuracy = test_acc / test_batches * 100\n",
    "# print max_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Р—Р°РјРµСЂ СЃРєРѕСЂРѕСЃС‚Рё СЃРµС‚Рё"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# weight = np.load('the weight of the networks/xenx_network_v_1_baseline_weight.npy')\n",
    "# lasagne.layers.set_all_param_values(dense_output, weight)\n",
    "# weight = lasagne.layers.get_all_param_values(dense_output)\n",
    "# np.save('the weight of the networks/xenx_network_v_1_baseline_weight.npy', weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = theano.compile.function([input_layer.input_var], y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_run_time = 0 \n",
    "result = 0 \n",
    "count = 1000\n",
    "full_start_time = time.time() \n",
    "for i in range(count):\n",
    "    start_time = time.time() \n",
    "    res = net(X_test[:1]) \n",
    "    run_time = time.time() - start_time \n",
    "    all_run_time += run_time \n",
    "    # print run_time \n",
    "    # print res \n",
    "    result += int(1/run_time) \n",
    "    # print int(1/run_time)\n",
    "run_time = time.time() - full_start_time\n",
    "print 'Result:' \n",
    "print '\\tRun time:\\t\\t{:.3f}'.format(run_time)\n",
    "print '\\tAverage time:\\t\\t{:.5f}'.format(all_run_time/count) \n",
    "print '\\tPer second:\\t\\t{:.5f}'.format(count/run_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
